{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Available?   True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Cuda Available?  \", use_cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_ode_solver(z0, t0, t1, f):\n",
    "    \"\"\"\n",
    "    Simplest Euler ODE Solver\n",
    "    z0: value at intial state\n",
    "    t0: intial state\n",
    "    t1: final state to be calculated\n",
    "    f: derivatve of a function with paramters z and p(aka parameter) with respect to t i.e f=dz/dt\n",
    "    \"\"\"\n",
    "    h_max = .05 # Random Smallest Possible difference \n",
    "    n_steps = math.ceil((abs(t1-t0)/h_max).max().item()) # Based on distance between t1 and t0, number of steps to be taken with step length of h_max\n",
    "\n",
    "    h = (t1 - t0)/n_steps # Approximated Smallest Possible Difference\n",
    "    t = t0\n",
    "    z = z0\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        z = z + h*f(z, t) # Euler Method: z_1 = z_0 + h*f(z, t)\n",
    "        t = t + h # Updating t0 to reach t1 with step size of h\n",
    "    \n",
    "    # When we reach t1 from t0, its possible an ODE solver also makes the z0 to reach z1(the output)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseODESolver(nn.Module):\n",
    "    \"\"\"\n",
    "    Base Class for Parameters based ODE Solver\n",
    "    \"\"\"\n",
    "\n",
    "    def forward_with_grad(self, z, t, grad_ouptuts):\n",
    "        \"\"\"\n",
    "        A custome method(not a method of nn.Module)\n",
    "        Helps with calculation of coefficients required for reverse-mode automatic differentiation\n",
    "        This will calculate the following:\n",
    "        - Vector field: df/dz\n",
    "        - Rate of change of output with change in parameters: df/dp\n",
    "        - Rate of change of output with change in layers: df/dt\n",
    "        \"\"\"\n",
    "        batch_size = z.size(0)\n",
    "\n",
    "        out = self.forward(z, t)\n",
    "        a = grad_ouptuts\n",
    "\n",
    "        # Calculate change in output with respect to z, t, p\n",
    "        ## here a is a jacobian matrix, which is used for vector-jacobian multiplication\n",
    "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
    "            (out,), (z, t) + tuple(self.parameters()),\n",
    "            grad_ouptuts=(a),\n",
    "            allow_unused=True, retain_graph=True\n",
    "        )\n",
    "        \n",
    "        if adfdp is not None:\n",
    "            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0)\n",
    "            adfdp = adfdp.expand(batch_size, -1) / batch_size\n",
    "\n",
    "        if adfdt is not None:\n",
    "            adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
    "        return out, adfdz, adfdt, adfdp\n",
    "    \n",
    "    def flatten_parameters(self):\n",
    "        p_shapes = []\n",
    "        flat_parameters = []\n",
    "        for p in self.parameters:\n",
    "            p_shapes.append(p.size())\n",
    "            flat_parameters.append(p.flatten())\n",
    "        return torch.cat(flat_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below encapsulates the forward and backward passes of a *Neural ODE*. We need to separate it from the main `torch.nn.Module` because a custom backward function cannot be implemented inside a Module, but it can be implemented inside `torch.autograd.Function`. This is a simple workaround.\n",
    "\n",
    "This function is fundamental to the entire Neural ODE method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackPropagation(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z0, t, flat_parameters, ode):\n",
    "        assert isinstance(ode, BaseODESolver)\n",
    "        bs, *z_shape = z0.size()\n",
    "        time_steps = t.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = torch.zeros(time_steps, bs, *z_shape).to(z0)\n",
    "            z[0] = z0\n",
    "\n",
    "            for t_i in range(time_steps - 1):\n",
    "                z0 = euler_ode_solver(z, t[t_i], t[t_i+1], ode)\n",
    "                z[t_i + 1] = z0\n",
    "        \n",
    "        ctx.func = ode\n",
    "        ctx.save_for_backward(t, z.clone(), flat_parameters)\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, dLdz):\n",
    "        \"\"\"\n",
    "        dLdz shape: time_steps, batch_size, *z_shape\n",
    "        \"\"\"\n",
    "        ode = ctx.func\n",
    "        t, z, flat_parameters = ctx.saved_tensors\n",
    "\n",
    "        time_steps, bs, *z_shape = z.size()\n",
    "        n_dim = np.prod(z_shape)\n",
    "        n_params = flat_parameters.size(0)\n",
    "\n",
    "        # Dynamics of augmented system to be calculated backwards in time\n",
    "        def augmented_dynamics(aug_z_i, t_i):\n",
    "            \"\"\"\n",
    "            Slices of Temporal tensors\n",
    "            aug_z_i: bs, n_dim*2 + n_params + 1\n",
    "            t_i: bs, 1\n",
    "            \"\"\"\n",
    "\n",
    "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]\n",
    "\n",
    "            # Squeeze z and a\n",
    "            z_i = z_i.view(bs, *z_shape)\n",
    "            a = a.view(bs, *z_shape)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                t_i = t_i.detach().requires_grad(True)\n",
    "                z_i = z_i.detach().requires_grad(True)\n",
    "\n",
    "                ode_output, adfdz, adfdt, adfdp = ode.forward_with_grad(z_i, t_i, grad_ouptuts=a)\n",
    "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n",
    "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n",
    "                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i)\n",
    "\n",
    "            ode_output = ode_output.view(bs, n_dim)\n",
    "            adfdz = adfdz.view(bs, n_dim)\n",
    "            return torch.cat((ode_output, -adfdz, -adfdp, -adfdt), dim=1)\n",
    "        \n",
    "        dLdz = dLdz.view(time_steps, bs, n_dim)\n",
    "        with torch.no_grad():\n",
    "            # Create Placeholder for each gradient, with respect to their size\n",
    "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
    "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
    "            adj_t = torch.zeros(time_steps, bs, 1).to(dLdz)\n",
    "\n",
    "            for i_t in range(time_steps-1, 0, -1):\n",
    "                z_i = z[i_t]\n",
    "                t_i = t[i_t]\n",
    "                out_i = ode(z_i, t_i).view(bs, n_dim)\n",
    "\n",
    "                # Compute direct gradients\n",
    "                dLdz_i = dLdz[i_t]\n",
    "                dLdt_i = torch.bmm(\n",
    "                    torch.transpose(\n",
    "                        dLdz_i.unsqueeze(-1), \n",
    "                        1,\n",
    "                        2\n",
    "                    ), out_i.unsqueeze(-1)\n",
    "                )[:, 0]\n",
    "\n",
    "                # Adjusting adjoints with direct gradients\n",
    "                adj_z += dLdz_i\n",
    "                adj_t[i_t] = adj_t[i_t] - dLdt_i\n",
    "\n",
    "                # Concatenate augmented Variable\n",
    "                aug_z = torch.cat(\n",
    "                    (z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]),\n",
    "                    dim=-1\n",
    "                )\n",
    "\n",
    "                # Solve augmented system backwards\n",
    "                aug_ans = euler_ode_solver(aug_z, t_i, t[i_t-1], augmented_dynamics)\n",
    "\n",
    "                # Unpack solved backwards augmented system\n",
    "                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n",
    "                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n",
    "                adj_t[i_t-1] = aug_ans[:, 2*n_dim+n_params:]\n",
    "\n",
    "                del aug_z, aug_ans\n",
    "\n",
    "            ## Adjust 0 time adjoint with direct gradients\n",
    "            # Compute direct gradients\n",
    "            dLdz_0 = dLdz[0]\n",
    "            dLdt = torch.bmm(\n",
    "                torch.transpose(dLdz_0.unsqueeze(-1), 1, 2),\n",
    "                out_i.unsqueeze(-1)\n",
    "            )[:, 0]\n",
    "\n",
    "            # Adjust adjoints\n",
    "            adj_z += dLdz_0\n",
    "            adj_t[0] = adj_t[0] - dLdt_0\n",
    "        return adj_z.view(bs, *z_shape), adj_t, adj_p, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Class to use Backpropagation with NeuralODE using **nn.Module** for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self, ode):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        assert isinstance(ode, BaseODESolver)\n",
    "        self.ode = ode\n",
    "\n",
    "    def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n",
    "        t = t.to(z0)\n",
    "        z = BackPropagation.apply(z0, t, self.ode.flatten_parameters(), self.ode)\n",
    "\n",
    "        return z if return_whole_sequence else z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
